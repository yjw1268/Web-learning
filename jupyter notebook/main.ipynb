{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DE_time', 'FE_time', 'BA_time', 'RPM']\n"
     ]
    }
   ],
   "source": [
    "# 初始化参数，获取列名\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "params = {}\n",
    "params['n_estimators'] = 100\n",
    "params['max_samples'] ='auto'\n",
    "params['contamination'] = 0.1\n",
    "params['max_features'] = 1.0\n",
    "\n",
    "params['nu'] = 0.001\n",
    "params['gamma']='auto'\n",
    "params['kernel'] ='poly'\n",
    "\n",
    "params['path'] = 'data/train/B01.csv'\n",
    "params['opath'] ='dataclean/B01.csv'\n",
    "try:\n",
    "    with open(params['path'],'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        head_row=next(reader)\n",
    "        data_attribute = []\n",
    "    for item in head_row:\n",
    "        data_attribute.append(item)\n",
    "    print(data_attribute)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.91511018e-01 -3.69818182e-02 -8.20842730e-03  1.79600000e+03]\n",
      " [ 1.50577365e-01  4.49945455e-02 -1.55316320e-02  1.79600000e+03]\n",
      " [-3.68727745e-02 -8.62909091e-03  3.82255193e-03  1.79600000e+03]\n",
      " ...\n",
      " [ 2.42353214e-01 -6.98545455e-03  7.06970920e-02  1.79600000e+03]\n",
      " [ 3.24870259e-03  1.06836364e-02  7.12604154e-02  1.79600000e+03]\n",
      " [-6.22126547e-02 -1.01700000e-01 -2.03198813e-02  1.79600000e+03]]\n",
      "14708\n"
     ]
    }
   ],
   "source": [
    "# dataclean\n",
    "tn = pd.read_csv(params['path']) \n",
    "tn.dropna(inplace=True)\n",
    "train = np.array(tn)\n",
    "train_x = np.array(train)\n",
    "print(train_x)\n",
    "# clf = IsolationForest(n_estimators=params['n_estimators'], \n",
    "#                       max_samples=params['max_samples'], \n",
    "#                       contamination=params['contamination'], \n",
    "#                       max_features=params['max_features'], \n",
    "#                       bootstrap=False, n_jobs=1, random_state=None, \n",
    "#                       verbose=0).fit(train_x)\n",
    "clf = svm.OneClassSVM(nu=params['nu'],\n",
    "              kernel=params['kernel'],\n",
    "              gamma=params['gamma']).fit(train_x)\n",
    "pred = clf.predict(train_x)\n",
    "print (pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14708\n"
     ]
    }
   ],
   "source": [
    "# 保存清洗后数据\n",
    "df = pd.DataFrame(pd.read_csv(params['path']))[0:pred.size]\n",
    "df['pred']=pred\n",
    "df2 = df[-df.pred.isin([-1])]\n",
    "df2 = df2.drop(['pred'],axis=1)\n",
    "data_out = df2.iloc[:,:].values\n",
    "print(int(df2.size/4))\n",
    "csvfile2 = open(params['opath'],'w',newline='')\n",
    "writer = csv.writer(csvfile2)\n",
    "writer.writerow(data_attribute)   #存属性\n",
    "m=len(data_out)\n",
    "for i in range(m):\n",
    "    writer.writerow(data_out[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182953\n",
      "362927\n",
      "14708\n",
      "14621\n",
      "14638\n",
      "14569\n",
      "14656\n",
      "14604\n",
      "7336\n",
      "7319\n",
      "7336\n",
      "7310\n",
      "7302\n",
      "7345\n",
      "7310\n",
      "7310\n",
      "7345\n",
      "7319\n",
      "7328\n",
      "7319\n",
      "7319\n",
      "7345\n",
      "14551\n",
      "14621\n",
      "14656\n",
      "14638\n",
      "14621\n",
      "14586\n",
      "14586\n",
      "14656\n",
      "14621\n",
      "7293\n",
      "7336\n",
      "7336\n",
      "7362\n",
      "363797\n",
      "7336\n",
      "7310\n",
      "7284\n",
      "14621\n",
      "14656\n",
      "14638\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 特征提取\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats,signal,fftpack\n",
    "import math\n",
    "from pywt import wavedec\n",
    "import traceback\n",
    "import sys\n",
    "columns_list=['time_mean','time_std','time_max','time_min','time_rms','time_ptp','time_median','time_iqr','time_pr','time_skew','time_kurtosis','time_var','time_amp',                'time_smr','time_wavefactor','time_peakfactor','time_pulse','time_margin','freq_mean','freq_std','freq_max','freq_min','freq_rms','freq_median',               'freq_iqr','freq_pr','freq_f2','freq_f3','freq_f4','freq_f5','freq_f6','freq_f7','freq_f8','ener_cA5','ener_cD1','ener_cD2','ener_cD3','ener_cD4',        'ener_cD5','ratio_cA5','ratio_cD1','ratio_cD2','ratio_cD3','ratio_cD4','ratio_cD5']\n",
    "columns_list1 = [a + '_DE' for a in columns_list]\n",
    "columns_list2 = [a + '_FE' for a in columns_list]\n",
    "columns_list3 = [a + '_BA' for a in columns_list]\n",
    "columns_list_final1 = columns_list1 + columns_list2+ columns_list3\n",
    "columns_list_final2 = columns_list1 + columns_list2\n",
    "params = {}\n",
    "params['len_piece']=50 #窗口长度\n",
    "windowlen=params['len_piece']\n",
    "\n",
    "\n",
    "def feature_get(filepath,windowlen):\n",
    "        df_data = pd.DataFrame(pd.read_csv(filepath))\n",
    "        if((df_data.columns.values=='BA_time').any()==False):\n",
    "            dfs = df_data.loc[:,['DE_time','FE_time']]\n",
    "        else:\n",
    "            dfs = df_data.loc[:,['DE_time','FE_time','BA_time']]\n",
    "        features_list=[]\n",
    "        print(len(dfs))\n",
    "        for i in range (0,len(dfs),windowlen):\n",
    "            if(int((len(dfs)-i)/windowlen)>=1): #舍去少量数据\n",
    "                df=dfs[i:i+params['len_piece']]\n",
    "                feature_list = []\n",
    "    #             print(df)\n",
    "                for i in df.columns:\n",
    "                    #----------  time-domain feature,18\n",
    "                    #依次为均值，标准差，最大值，最小值，均方根，峰峰值，中位数，四分位差，百分位差，偏度，峰度，方差，整流平均值，方根幅值，波形因子，峰值因子，脉冲值，裕度\n",
    "                    df_line = df[i]\n",
    "    #                 print(df_line)\n",
    "                    time_mean = df_line.mean()\n",
    "                    time_std = df_line.std()\n",
    "                    time_max = df_line.max()\n",
    "                    time_min = df_line.min()\n",
    "                    time_rms = np.sqrt(np.square(df_line).mean())\n",
    "                    time_ptp = time_max-time_min \n",
    "                    time_median = np.median(df_line)\n",
    "                    time_iqr = np.percentile(df_line,75)-np.percentile(df_line,25)\n",
    "                    time_pr = np.percentile(df_line,90)-np.percentile(df_line,10)\n",
    "                    time_skew = stats.skew(df_line)\n",
    "                    time_kurtosis = stats.kurtosis(df_line)\n",
    "                    time_var = np.var(df_line)\n",
    "                    time_amp = np.abs(df_line).mean()\n",
    "                    time_smr = np.square(np.sqrt(np.abs(df_line)).mean())\n",
    "                    #下面四个特征需要注意分母为0或接近0问题，可能会发生报错\n",
    "                    time_wavefactor = time_rms/time_amp\n",
    "                    time_peakfactor = time_max/time_rms\n",
    "                    time_pulse = time_max/time_amp\n",
    "                    time_margin = time_max/time_smr\n",
    "                    #----------  freq-domain feature,15\n",
    "                    #采样频率25600Hz\n",
    "                    df_fftline = fftpack.fft(df[i])\n",
    "                    freq_fftline = fftpack.fftfreq(len(df[i]),1/25600)\n",
    "                    df_fftline = abs(df_fftline[freq_fftline>=0])\n",
    "                    freq_fftline = freq_fftline[freq_fftline>=0]\n",
    "                    #基本特征,依次为均值，标准差，最大值，最小值，均方根，中位数，四分位差，百分位差\n",
    "                    freq_mean = df_fftline.mean()\n",
    "                    freq_std = df_fftline.std()\n",
    "                    freq_max = df_fftline.max()\n",
    "                    freq_min = df_fftline.min()\n",
    "                    freq_rms = np.sqrt(np.square(df_fftline).mean())\n",
    "                    freq_median = np.median(df_fftline)\n",
    "                    freq_iqr = np.percentile(df_fftline,75)-np.percentile(df_fftline,25)\n",
    "                    freq_pr = np.percentile(df_fftline,90)-np.percentile(df_fftline,10)\n",
    "                    #f2 f3 f4反映频谱集中程度\n",
    "                    freq_f2 = np.square((df_fftline-freq_mean)).sum()/(len(df_fftline)-1)\n",
    "                    freq_f3 = pow((df_fftline-freq_mean),3).sum()/(len(df_fftline)*pow(freq_f2,1.5))\n",
    "                    freq_f4 = pow((df_fftline-freq_mean),4).sum()/(len(df_fftline)*pow(freq_f2,2))\n",
    "                    #f5 f6 f7 f8反映主频带位置\n",
    "                    freq_f5 = np.multiply(freq_fftline,df_fftline).sum()/df_fftline.sum()\n",
    "                    freq_f6 = np.sqrt(np.multiply(np.square(freq_fftline),df_fftline).sum())/df_fftline.sum()\n",
    "                    freq_f7 = np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum())/np.multiply(np.square(freq_fftline),df_fftline).sum()\n",
    "                    freq_f8 = np.multiply(np.square(freq_fftline),df_fftline).sum()/np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum()*df_fftline.sum())\n",
    "                    #----------  timefreq-domain feature,12\n",
    "                    #5级小波变换，最后输出6个能量特征和其归一化能量特征\n",
    "                    cA5, cD5, cD4, cD3, cD2, cD1 = wavedec(df[i], 'db10', level=5)\n",
    "                    ener_cA5 = np.square(cA5).sum()\n",
    "                    ener_cD5 = np.square(cD5).sum()\n",
    "                    ener_cD4 = np.square(cD4).sum()\n",
    "                    ener_cD3 = np.square(cD3).sum()\n",
    "                    ener_cD2 = np.square(cD2).sum()\n",
    "                    ener_cD1 = np.square(cD1).sum()\n",
    "                    ener = ener_cA5 + ener_cD1 + ener_cD2 + ener_cD3 + ener_cD4 + ener_cD5\n",
    "                    ratio_cA5 = ener_cA5/ener\n",
    "                    ratio_cD5 = ener_cD5/ener\n",
    "                    ratio_cD4 = ener_cD4/ener\n",
    "                    ratio_cD3 = ener_cD3/ener\n",
    "                    ratio_cD2 = ener_cD2/ener\n",
    "                    ratio_cD1 = ener_cD1/ener\n",
    "                    feature_list.extend([time_mean,time_std,time_max,time_min,time_rms,time_ptp,time_median,time_iqr,time_pr,time_skew,time_kurtosis,time_var,time_amp,\n",
    "                                     time_smr,time_wavefactor,time_peakfactor,time_pulse,time_margin,freq_mean,freq_std,freq_max,freq_min,freq_rms,freq_median,\n",
    "                                     freq_iqr,freq_pr,freq_f2,freq_f3,freq_f4,freq_f5,freq_f6,freq_f7,freq_f8,ener_cA5,ener_cD1,ener_cD2,ener_cD3,ener_cD4,ener_cD5,\n",
    "                                     ratio_cA5,ratio_cD1,ratio_cD2,ratio_cD3,ratio_cD4,ratio_cD5])\n",
    "                features_list.append(feature_list)\n",
    "        return features_list\n",
    "\n",
    "# 约定normal(NORMAL), ball(B), outer race(OR), inner race(IR)的预测输出标签为0, 1, 2, 3。\n",
    "# NORMAL:0\n",
    "for i in range(1,3):\n",
    "    params['data_path'] = 'data/train/NORMAL0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/NORMAL0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final2)\n",
    "    result['label']=0\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "\n",
    "# B:1    \n",
    "for i in range(1,7):\n",
    "    params['data_path'] = 'data/train/B0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/B0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final1)\n",
    "    result['label']=1\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "# OR:2    \n",
    "for i in range(1,15):\n",
    "    if(i<=9):\n",
    "        params['data_path'] = 'data/train/OR0'+str(i)+'.csv'\n",
    "        params['opath'] ='datafeature/OR0'+str(i)+'_feature.csv'\n",
    "    else:\n",
    "        params['data_path'] = 'data/train/OR'+str(i)+'.csv'\n",
    "        params['opath'] ='datafeature/OR'+str(i)+'_feature.csv'        \n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final1)\n",
    "    result['label']=2\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "# IR:3\n",
    "for i in range(1,7):\n",
    "    params['data_path'] = 'data/train/IR0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/IR0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final1)\n",
    "    result['label']=3\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "\n",
    "# TEST:-1\n",
    "for i in range(1,15):\n",
    "    if(i<=9):\n",
    "        params['data_path'] = 'data/test/TEST0'+str(i)+'.csv'\n",
    "        params['opath'] ='testfeature/TEST0'+str(i)+'_feature.csv'\n",
    "    else:\n",
    "        params['data_path'] = 'data/test/TEST'+str(i)+'.csv'\n",
    "        params['opath'] ='testfeature/TEST'+str(i)+'_feature.csv'        \n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    if(i==8):\n",
    "        result = pd.DataFrame(fea,columns = columns_list_final2)\n",
    "    else:\n",
    "        result = pd.DataFrame(fea,columns = columns_list_final1)\n",
    "    result['label']=-1\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 合并训练集\n",
    "import os\n",
    "import pandas as pd\n",
    "filePath='datafeature'\n",
    "outputfile='train.csv'\n",
    "fileList = os.listdir(filePath)\n",
    "# print(fileList)\n",
    "# print(len(fileList))\n",
    "path = os.path.join(filePath, fileList[0])\n",
    "df = pd.read_csv(path)\n",
    "df.to_csv(outputfile,index=False, header=True)\n",
    "for i in range (1,len(fileList)):\n",
    "    path = os.path.join(filePath, fileList[i])\n",
    "#     print(path)\n",
    "    df = pd.read_csv(path)\n",
    "    df.to_csv(outputfile, mode='a', index=False, header=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 空值填充0，主要是对normal类\n",
    "test8=pd.read_csv('testfeature/TEST08_feature.csv')\n",
    "outputfile='train.csv'\n",
    "df = pd.read_csv(outputfile)\n",
    "for i in df.columns.values:\n",
    "    df[i].fillna(0, inplace=True)\n",
    "df.to_csv(outputfile,index=False,header=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# LGBM分类\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    model_lgb_default = lgb.LGBMClassifier(learning_rate=0.03,random_state=None)\n",
    "    # 模型训练\n",
    "    model_lgb_default.fit(train_data_x, train_data_y)\n",
    "    joblib.dump(model_lgb_default, 'model/lightgbm_model.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数 ： {'n_neighbors': 7}\n",
      "最佳结果： 0.9300702080032837\n",
      "最佳预估器： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
      "                     weights='uniform')\n",
      "交叉验证结果 {'mean_fit_time': array([0.26179987, 0.26257551, 0.24859101, 0.25480616, 0.25132751]), 'std_fit_time': array([0.00696272, 0.00411099, 0.00332768, 0.00389304, 0.00315364]), 'mean_score_time': array([0.26977861, 0.32360727, 0.35081458, 0.37772977, 0.38821179]), 'std_score_time': array([0.01023192, 0.01789866, 0.02159024, 0.02324307, 0.00867673]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 9],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 9}], 'split0_test_score': array([0.92470507, 0.93372658, 0.93962526, 0.94031922, 0.93858432]), 'split1_test_score': array([0.91047883, 0.91984733, 0.93129771, 0.93060375, 0.93025677]), 'split2_test_score': array([0.91461298, 0.92572024, 0.92676154, 0.92814995, 0.92745574]), 'split3_test_score': array([0.90697674, 0.91530719, 0.91600139, 0.92120791, 0.92190212]), 'mean_test_score': array([0.91419341, 0.92365033, 0.92842147, 0.93007021, 0.92954974]), 'std_test_score': array([0.00664359, 0.00688996, 0.0085266 , 0.00684753, 0.00602068]), 'rank_test_score': array([5, 4, 3, 1, 2])}\n",
      "准确度为： 0.9251012145748988\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# knn分类\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_data_x, train_data_y, test_size=0.3, random_state=0)\n",
    "    # 传入到knn进行训练，得到模型\n",
    "    classifier = KNeighborsClassifier()\n",
    "    param_dict = {\"n_neighbors\":[1,3,5,7,9]}\n",
    "   #加入网格搜索和交叉验证\n",
    "    classifier = GridSearchCV(classifier,param_grid=param_dict,cv=4)\n",
    "    classifier.fit(train_x,train_y)\n",
    " \n",
    "    y_predict = classifier.predict(valid_x)\n",
    "    print(\"最佳参数 ：\",classifier.best_params_)\n",
    "    print(\"最佳结果：\",classifier.best_score_)\n",
    "    print(\"最佳预估器：\",classifier.best_estimator_)\n",
    "    print(\"交叉验证结果\",classifier.cv_results_)\n",
    "    print(\"准确度为：\", classifier.score(valid_x,valid_y))\n",
    "    joblib.dump(classifier, 'model/knn.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjw98\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Multiclass objective and metrics don't match\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   1663\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1664\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1038\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    840\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[1;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m    888\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Multiclass objective and metrics don't match",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fa0b8827293e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-fa0b8827293e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m                          lambda_l2= 0)\n\u001b[0;32m     49\u001b[0m     \u001b[0mgsearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgbm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best score: %0.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters set:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    798\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    801\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreduced_valid_sets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mvalid_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_reverse_update_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams_back_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m             \u001b[0m_safe_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_DatasetUpdateParam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_dict_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 超参数搜索（BUGing）\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_data_x, train_data_y, test_size=0.333, random_state=0)\n",
    "    train = lgb.Dataset(train_x, train_y)\n",
    "    valid = lgb.Dataset(valid_x, valid_y, reference=train)\n",
    "    parameters = {\n",
    "              'max_depth': [15, 20, 25, 30, 35],\n",
    "              'learning_rate': [0.05, 0.1, 0.15],\n",
    "              'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_freq': [2, 4, 5, 6, 8],\n",
    "              'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "              'lambda_l2': [0, 10, 15, 35, 40],\n",
    "              'cat_smooth': [1, 10, 15, 20, 35]\n",
    "    }\n",
    "    gbm = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         objective = 'binary',\n",
    "                         metric = 'auc',\n",
    "                         verbose = 0,\n",
    "                         learning_rate = 0.01,\n",
    "                         num_leaves = 35,\n",
    "                         feature_fraction=0.8,\n",
    "                         bagging_fraction= 0.9,\n",
    "                         bagging_freq= 8,\n",
    "                         lambda_l1= 0.6,\n",
    "                         lambda_l2= 0)\n",
    "    gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "    gsearch.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = gsearch.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "#     joblib.dump(model_lgb_default, 'model/lightgbm_model.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\yjw98\\\\AppData\\\\Local\\\\Temp\\\\tmprvyp_s0v', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022396E105C0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yjw98\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adagrad.py:108: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt.\n",
      "INFO:tensorflow:loss = 188.21637, step = 0\n",
      "INFO:tensorflow:global_step/sec: 22.7525\n",
      "INFO:tensorflow:loss = 15.600134, step = 100 (4.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9722\n",
      "INFO:tensorflow:loss = 8.686536, step = 200 (3.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9133\n",
      "INFO:tensorflow:loss = 8.234123, step = 300 (3.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.1237\n",
      "INFO:tensorflow:loss = 9.70068, step = 400 (3.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.7648\n",
      "INFO:tensorflow:loss = 9.898171, step = 500 (3.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.5449\n",
      "INFO:tensorflow:loss = 6.854823, step = 600 (3.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.4899\n",
      "INFO:tensorflow:loss = 6.9970202, step = 700 (4.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.2552\n",
      "INFO:tensorflow:loss = 4.8064647, step = 800 (4.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.7423\n",
      "INFO:tensorflow:loss = 5.0705347, step = 900 (4.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0253\n",
      "INFO:tensorflow:loss = 3.9805002, step = 1000 (4.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2494\n",
      "INFO:tensorflow:loss = 3.3877761, step = 1100 (4.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2838\n",
      "INFO:tensorflow:loss = 3.8764, step = 1200 (4.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.6989\n",
      "INFO:tensorflow:loss = 2.6453857, step = 1300 (4.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.6385\n",
      "INFO:tensorflow:loss = 2.3827667, step = 1400 (4.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0664\n",
      "INFO:tensorflow:loss = 1.5380056, step = 1500 (4.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.6494\n",
      "INFO:tensorflow:loss = 1.7153606, step = 1600 (4.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.9358\n",
      "INFO:tensorflow:loss = 2.3720276, step = 1700 (4.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.9779\n",
      "INFO:tensorflow:loss = 2.220849, step = 1800 (4.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.1047\n",
      "INFO:tensorflow:loss = 1.8326015, step = 1900 (4.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7141\n",
      "INFO:tensorflow:loss = 1.4506923, step = 2000 (4.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7015\n",
      "INFO:tensorflow:loss = 2.0751512, step = 2100 (4.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7065\n",
      "INFO:tensorflow:loss = 1.8378433, step = 2200 (4.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8739\n",
      "INFO:tensorflow:loss = 2.7973225, step = 2300 (4.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.6857\n",
      "INFO:tensorflow:loss = 1.6147878, step = 2400 (4.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7682\n",
      "INFO:tensorflow:loss = 1.8338826, step = 2500 (4.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.801\n",
      "INFO:tensorflow:loss = 2.9813786, step = 2600 (4.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.972\n",
      "INFO:tensorflow:loss = 2.4067278, step = 2700 (4.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7835\n",
      "INFO:tensorflow:loss = 2.1770523, step = 2800 (4.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.1444\n",
      "INFO:tensorflow:loss = 1.8627573, step = 2900 (4.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.4408\n",
      "INFO:tensorflow:loss = 1.6845255, step = 3000 (4.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.6458\n",
      "INFO:tensorflow:loss = 1.5247922, step = 3100 (4.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.3803\n",
      "INFO:tensorflow:loss = 1.4409893, step = 3200 (4.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8999\n",
      "INFO:tensorflow:loss = 1.5193228, step = 3300 (4.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8939\n",
      "INFO:tensorflow:loss = 1.6498088, step = 3400 (4.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7654\n",
      "INFO:tensorflow:loss = 1.9315921, step = 3500 (4.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.6531\n",
      "INFO:tensorflow:loss = 1.0896642, step = 3600 (4.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.386\n",
      "INFO:tensorflow:loss = 1.3345793, step = 3700 (4.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.5186\n",
      "INFO:tensorflow:loss = 1.3848604, step = 3800 (4.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8154\n",
      "INFO:tensorflow:loss = 0.98060155, step = 3900 (4.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.5572\n",
      "INFO:tensorflow:loss = 2.098597, step = 4000 (4.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.2276\n",
      "INFO:tensorflow:loss = 1.6529269, step = 4100 (4.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0909\n",
      "INFO:tensorflow:loss = 1.6313993, step = 4200 (4.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.6635\n",
      "INFO:tensorflow:loss = 1.2097167, step = 4300 (4.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.1864\n",
      "INFO:tensorflow:loss = 1.276607, step = 4400 (4.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2575\n",
      "INFO:tensorflow:loss = 1.1995223, step = 4500 (4.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0173\n",
      "INFO:tensorflow:loss = 1.1122075, step = 4600 (4.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.7122\n",
      "INFO:tensorflow:loss = 1.728448, step = 4700 (4.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.2853\n",
      "INFO:tensorflow:loss = 0.8884076, step = 4800 (4.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.4651\n",
      "INFO:tensorflow:loss = 1.3772345, step = 4900 (4.660 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.6469543.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x22398daaba8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to use tensorflow DNN\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "    # 将输入转换为数据集。\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # 如果在训练模式下混淆并重复数据。\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "# 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data_y=[]\n",
    "for i in train_data['label']:\n",
    "    train_data_y.append(int(i))\n",
    "# 除去标签的所有列就是特征\n",
    "train_data_x = train_data.drop(['label'], axis=1)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_data_x, train_data_y, test_size=0.25, random_state=0)\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # 隐层所含结点数量分别为 30 和 10.\n",
    "    hidden_units=[30, 10],\n",
    "    # 模型必须从三个类别中做出选择。\n",
    "    n_classes=4)\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train_x, train_y, training=True),\n",
    "    steps=5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-03-26T09:53:48Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-03-26-09:53:49\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.86665046, average_loss = 1.5567482, global_step = 5000, loss = 1.4822452\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "\n",
      "Test set accuracy: 0.867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估train_test_split中划分的测试集\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(test_x, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmprvyp_s0v\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "test_1 [0, 252, 28, 11] 1\n",
      "test_2 [0, 245, 46, 2] 1\n",
      "test_3 [0, 69, 136, 87] 2\n",
      "test_4 [0, 0, 136, 9] 2\n",
      "test_5 [8, 65, 45, 28] 1\n",
      "test_6 [1, 3, 48, 94] 3\n",
      "test_7 [0, 57, 45, 45] 1\n",
      "test_8 [10, 35, 59, 42] 2\n",
      "test_9 [1, 127, 18, 0] 1\n",
      "test_10 [0, 7, 84, 54] 2\n",
      "test_11 [0, 37, 145, 110] 2\n",
      "test_12 [0, 0, 0, 293] 3\n",
      "test_13 [0, 213, 63, 16] 1\n"
     ]
    }
   ],
   "source": [
    "# 由tf模型生成预测\n",
    "expected = ['NORMAL', 'B', 'OR','IR']\n",
    "test1=pd.read_csv('testfeature/TEST01_feature.csv')\n",
    "test2=pd.read_csv('testfeature/TEST02_feature.csv')\n",
    "test3=pd.read_csv('testfeature/TEST03_feature.csv')\n",
    "test4=pd.read_csv('testfeature/TEST04_feature.csv')\n",
    "test5=pd.read_csv('testfeature/TEST05_feature.csv')\n",
    "test6=pd.read_csv('testfeature/TEST06_feature.csv')\n",
    "test7=pd.read_csv('testfeature/TEST07_feature.csv')\n",
    "test8=pd.read_csv('testfeature/TEST08_feature.csv')\n",
    "test9=pd.read_csv('testfeature/TEST09_feature.csv')\n",
    "test10=pd.read_csv('testfeature/TEST10_feature.csv')\n",
    "test11=pd.read_csv('testfeature/TEST11_feature.csv')\n",
    "test12=pd.read_csv('testfeature/TEST12_feature.csv')\n",
    "test13=pd.read_csv('testfeature/TEST13_feature.csv')\n",
    "test14=pd.read_csv('testfeature/TEST14_feature.csv')\n",
    "def input_fn(features, batch_size=256):\n",
    "    \"\"\"An input function for prediction.\"\"\"\n",
    "    # 将输入转换为无标签数据集。\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "i=1\n",
    "testsets=[test1,test2,test3,test4,test5,test6,test7,test9,test10,test11,test12,test13,test14]\n",
    "names=[]\n",
    "predlists=[]\n",
    "ans=[]\n",
    "for test_i in testsets:\n",
    "    test_name = 'test_' + str(i)\n",
    "    i+=1\n",
    "    test_j=test_i.drop(['label'], axis=1)\n",
    "    predict_x=test_i\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda: input_fn(predict_x))\n",
    "    y_pred_binary=[]\n",
    "    for pred in predictions:\n",
    "        y_pred_binary.append(pred['class_ids'][0])\n",
    "    predlist=[y_pred_binary.count(0), y_pred_binary.count(1),y_pred_binary.count(2),y_pred_binary.count(3)]\n",
    "    pred=predlist.index(max(predlist))\n",
    "    names.append(test_name)\n",
    "    ans.append(pred)\n",
    "    predlists.append(predlist)\n",
    "    \n",
    "for name,predlist,pred in zip(names,predlists,ans):\n",
    "    print(name,predlist,pred)\n",
    "    \n",
    "# for pred_dict, expec in zip(predictions, expected):\n",
    "#     class_id = pred_dict['class_ids'][0]\n",
    "#     probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "#     print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\n",
    "#         expected[class_id], 100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of test is:\n",
      "test_1 [2, 287, 1, 1] 1\n",
      "The result of test is:\n",
      "test_2 [12, 278, 3, 0] 1\n",
      "The result of test is:\n",
      "test_3 [91, 105, 20, 76] 1\n",
      "The result of test is:\n",
      "test_4 [5, 0, 134, 6] 2\n",
      "The result of test is:\n",
      "test_5 [30, 47, 67, 2] 2\n",
      "The result of test is:\n",
      "test_6 [13, 0, 114, 19] 2\n",
      "The result of test is:\n",
      "test_7 [25, 55, 57, 10] 2\n",
      "The result of test is:\n",
      "test_8 [26, 30, 84, 6] 2\n",
      "The result of test is:\n",
      "test_9 [23, 59, 63, 1] 2\n",
      "The result of test is:\n",
      "test_10 [8, 0, 125, 12] 2\n",
      "The result of test is:\n",
      "test_11 [42, 25, 10, 215] 3\n",
      "The result of test is:\n",
      "test_12 [0, 0, 3, 290] 3\n",
      "The result of test is:\n",
      "test_13 [10, 278, 4, 0] 1\n"
     ]
    }
   ],
   "source": [
    "# 预测结果\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def judge(input_pred):\n",
    "    return_pred = list(np.zeros(len(input_pred)))\n",
    "    for i in range(0, len(input_pred)):\n",
    "        if (input_pred[i][0] > 0.5):\n",
    "            return_pred[i] = 0\n",
    "        elif(input_pred[i][1]>0.5):\n",
    "            return_pred[i] = 1\n",
    "        elif(input_pred[i][2]>0.5):\n",
    "            return_pred[i] = 2\n",
    "        elif(input_pred[i][3]>0.5):\n",
    "            return_pred[i] = 3\n",
    "    return return_pred\n",
    "    \n",
    "def score(predlist ,pred):\n",
    "    # score = [0.3×f1score(class1) + 0.3×f1score(class2) + 0.3×f1score(class3) + 0.1×f1score(class0)]*100\n",
    "    pass\n",
    "\n",
    "def test_lightgbm():\n",
    "    # 加载模型\n",
    "    model = joblib.load('model/lightgbm_model.model')\n",
    "#     model = joblib.load('model/knn.model')\n",
    "    test1=pd.read_csv('testfeature/TEST01_feature.csv')\n",
    "    test2=pd.read_csv('testfeature/TEST02_feature.csv')\n",
    "    test3=pd.read_csv('testfeature/TEST03_feature.csv')\n",
    "    test4=pd.read_csv('testfeature/TEST04_feature.csv')\n",
    "    test5=pd.read_csv('testfeature/TEST05_feature.csv')\n",
    "    test6=pd.read_csv('testfeature/TEST06_feature.csv')\n",
    "    test7=pd.read_csv('testfeature/TEST07_feature.csv')\n",
    "    test8=pd.read_csv('testfeature/TEST08_feature.csv')\n",
    "    test9=pd.read_csv('testfeature/TEST09_feature.csv')\n",
    "    test10=pd.read_csv('testfeature/TEST10_feature.csv')\n",
    "    test11=pd.read_csv('testfeature/TEST11_feature.csv')\n",
    "    test12=pd.read_csv('testfeature/TEST12_feature.csv')\n",
    "    test13=pd.read_csv('testfeature/TEST13_feature.csv')\n",
    "    test14=pd.read_csv('testfeature/TEST14_feature.csv')\n",
    "    i=1\n",
    "    for test_i in [test1,test2,test3,test4,test5,test6,test7,test9,test10,test11,test12,test13,test14]:\n",
    "        test_name = 'test_' + str(i)\n",
    "        i+=1\n",
    "        print('The result of test is:')\n",
    "        test_j=test_i.drop(['label'], axis=1)\n",
    "        y_pred = model.predict_proba(test_j)\n",
    "        y_pred_binary = judge(y_pred)\n",
    "        predlist=[y_pred_binary.count(0), y_pred_binary.count(1),y_pred_binary.count(2),y_pred_binary.count(3)]\n",
    "        pred=predlist.index(max(predlist))\n",
    "        print(test_name,predlist ,pred)\n",
    "\n",
    "\n",
    "        \n",
    "test_lightgbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
