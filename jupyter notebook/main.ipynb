{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DE_time', 'FE_time', 'BA_time', 'RPM']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "params = {}\n",
    "params['n_estimators'] = 100\n",
    "params['max_samples'] ='auto'\n",
    "params['contamination'] = 0.1\n",
    "params['max_features'] = 1.0\n",
    "\n",
    "params['nu'] = 0.001\n",
    "params['gamma']='auto'\n",
    "params['kernel'] ='poly'\n",
    "\n",
    "params['path'] = 'data/train/B01.csv'\n",
    "params['opath'] ='dataclean/B01.csv'\n",
    "try:\n",
    "    with open(params['path'],'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        head_row=next(reader)\n",
    "        data_attribute = []\n",
    "    for item in head_row:\n",
    "        data_attribute.append(item)\n",
    "    print(data_attribute)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.91511018e-01 -3.69818182e-02 -8.20842730e-03  1.79600000e+03]\n",
      " [ 1.50577365e-01  4.49945455e-02 -1.55316320e-02  1.79600000e+03]\n",
      " [-3.68727745e-02 -8.62909091e-03  3.82255193e-03  1.79600000e+03]\n",
      " ...\n",
      " [ 2.42353214e-01 -6.98545455e-03  7.06970920e-02  1.79600000e+03]\n",
      " [ 3.24870259e-03  1.06836364e-02  7.12604154e-02  1.79600000e+03]\n",
      " [-6.22126547e-02 -1.01700000e-01 -2.03198813e-02  1.79600000e+03]]\n",
      "14708\n"
     ]
    }
   ],
   "source": [
    "tn = pd.read_csv(params['path']) \n",
    "tn.dropna(inplace=True)\n",
    "train = np.array(tn)\n",
    "train_x = np.array(train)\n",
    "print(train_x)\n",
    "# clf = IsolationForest(n_estimators=params['n_estimators'], \n",
    "#                       max_samples=params['max_samples'], \n",
    "#                       contamination=params['contamination'], \n",
    "#                       max_features=params['max_features'], \n",
    "#                       bootstrap=False, n_jobs=1, random_state=None, \n",
    "#                       verbose=0).fit(train_x)\n",
    "clf = svm.OneClassSVM(nu=params['nu'],\n",
    "              kernel=params['kernel'],\n",
    "              gamma=params['gamma']).fit(train_x)\n",
    "pred = clf.predict(train_x)\n",
    "print (pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14708\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.read_csv(params['path']))[0:pred.size]\n",
    "df['pred']=pred\n",
    "df2 = df[-df.pred.isin([-1])]\n",
    "df2 = df2.drop(['pred'],axis=1)\n",
    "data_out = df2.iloc[:,:].values\n",
    "print(int(df2.size/4))\n",
    "csvfile2 = open(params['opath'],'w',newline='')\n",
    "writer = csv.writer(csvfile2)\n",
    "writer.writerow(data_attribute)   #存属性\n",
    "m=len(data_out)\n",
    "for i in range(m):\n",
    "    writer.writerow(data_out[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjw98\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pywt\\_multilevel.py:45: UserWarning: Level value of 5 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362927\n",
      "14708\n",
      "14621\n",
      "14638\n",
      "14569\n",
      "14656\n",
      "14604\n",
      "7336\n",
      "7319\n",
      "7336\n",
      "7310\n",
      "7302\n",
      "7345\n",
      "7310\n",
      "7310\n",
      "7345\n",
      "7319\n",
      "7328\n",
      "7319\n",
      "7319\n",
      "7345\n",
      "14551\n",
      "14621\n",
      "14656\n",
      "14638\n",
      "14621\n",
      "14586\n",
      "14586\n",
      "14656\n",
      "14621\n",
      "7293\n",
      "7336\n",
      "7336\n",
      "7362\n",
      "363797\n",
      "7336\n",
      "7310\n",
      "7284\n",
      "14621\n",
      "14656\n",
      "14638\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats,signal,fftpack\n",
    "import math\n",
    "from pywt import wavedec\n",
    "import traceback\n",
    "import sys\n",
    "columns_list=['time_mean','time_std','time_max','time_min','time_rms','time_ptp','time_median','time_iqr','time_pr','time_skew','time_kurtosis','time_var','time_amp',                'time_smr','time_wavefactor','time_peakfactor','time_pulse','time_margin','freq_mean','freq_std','freq_max','freq_min','freq_rms','freq_median',               'freq_iqr','freq_pr','freq_f2','freq_f3','freq_f4','freq_f5','freq_f6','freq_f7','freq_f8','ener_cA5','ener_cD1','ener_cD2','ener_cD3','ener_cD4',        'ener_cD5','ratio_cA5','ratio_cD1','ratio_cD2','ratio_cD3','ratio_cD4','ratio_cD5']\n",
    "columns_list1 = [a + '_DE' for a in columns_list]\n",
    "columns_list2 = [a + '_FE' for a in columns_list]\n",
    "# columns_list3 = [a + '_BA' for a in columns_list]\n",
    "# columns_list_final = columns_list1 + columns_list2+ columns_list3\n",
    "columns_list_final = columns_list1 + columns_list2\n",
    "params = {}\n",
    "params['len_piece']=100 #窗口长度\n",
    "windowlen=params['len_piece']\n",
    "\n",
    "\n",
    "def feature_get(filepath,windowlen):\n",
    "        df_data = pd.DataFrame(pd.read_csv(filepath))\n",
    "#         dfs = df_data.loc[:,['DE_time','FE_time','BA_time']]\n",
    "        dfs = df_data.loc[:,['DE_time','FE_time']]\n",
    "        features_list=[]\n",
    "        print(len(dfs))\n",
    "        for i in range (0,len(dfs),windowlen):\n",
    "            if(int((len(dfs)-i)/100)>=1): #舍去少量数据\n",
    "                df=dfs[i:i+params['len_piece']]\n",
    "                feature_list = []\n",
    "    #             print(df)\n",
    "                for i in df.columns:\n",
    "                    #----------  time-domain feature,18\n",
    "                    #依次为均值，标准差，最大值，最小值，均方根，峰峰值，中位数，四分位差，百分位差，偏度，峰度，方差，整流平均值，方根幅值，波形因子，峰值因子，脉冲值，裕度\n",
    "                    df_line = df[i]\n",
    "    #                 print(df_line)\n",
    "                    time_mean = df_line.mean()\n",
    "                    time_std = df_line.std()\n",
    "                    time_max = df_line.max()\n",
    "                    time_min = df_line.min()\n",
    "                    time_rms = np.sqrt(np.square(df_line).mean())\n",
    "                    time_ptp = time_max-time_min \n",
    "                    time_median = np.median(df_line)\n",
    "                    time_iqr = np.percentile(df_line,75)-np.percentile(df_line,25)\n",
    "                    time_pr = np.percentile(df_line,90)-np.percentile(df_line,10)\n",
    "                    time_skew = stats.skew(df_line)\n",
    "                    time_kurtosis = stats.kurtosis(df_line)\n",
    "                    time_var = np.var(df_line)\n",
    "                    time_amp = np.abs(df_line).mean()\n",
    "                    time_smr = np.square(np.sqrt(np.abs(df_line)).mean())\n",
    "                    #下面四个特征需要注意分母为0或接近0问题，可能会发生报错\n",
    "                    time_wavefactor = time_rms/time_amp\n",
    "                    time_peakfactor = time_max/time_rms\n",
    "                    time_pulse = time_max/time_amp\n",
    "                    time_margin = time_max/time_smr\n",
    "                    #----------  freq-domain feature,15\n",
    "                    #采样频率25600Hz\n",
    "                    df_fftline = fftpack.fft(df[i])\n",
    "                    freq_fftline = fftpack.fftfreq(len(df[i]),1/25600)\n",
    "                    df_fftline = abs(df_fftline[freq_fftline>=0])\n",
    "                    freq_fftline = freq_fftline[freq_fftline>=0]\n",
    "                    #基本特征,依次为均值，标准差，最大值，最小值，均方根，中位数，四分位差，百分位差\n",
    "                    freq_mean = df_fftline.mean()\n",
    "                    freq_std = df_fftline.std()\n",
    "                    freq_max = df_fftline.max()\n",
    "                    freq_min = df_fftline.min()\n",
    "                    freq_rms = np.sqrt(np.square(df_fftline).mean())\n",
    "                    freq_median = np.median(df_fftline)\n",
    "                    freq_iqr = np.percentile(df_fftline,75)-np.percentile(df_fftline,25)\n",
    "                    freq_pr = np.percentile(df_fftline,90)-np.percentile(df_fftline,10)\n",
    "                    #f2 f3 f4反映频谱集中程度\n",
    "                    freq_f2 = np.square((df_fftline-freq_mean)).sum()/(len(df_fftline)-1)\n",
    "                    freq_f3 = pow((df_fftline-freq_mean),3).sum()/(len(df_fftline)*pow(freq_f2,1.5))\n",
    "                    freq_f4 = pow((df_fftline-freq_mean),4).sum()/(len(df_fftline)*pow(freq_f2,2))\n",
    "                    #f5 f6 f7 f8反映主频带位置\n",
    "                    freq_f5 = np.multiply(freq_fftline,df_fftline).sum()/df_fftline.sum()\n",
    "                    freq_f6 = np.sqrt(np.multiply(np.square(freq_fftline),df_fftline).sum())/df_fftline.sum()\n",
    "                    freq_f7 = np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum())/np.multiply(np.square(freq_fftline),df_fftline).sum()\n",
    "                    freq_f8 = np.multiply(np.square(freq_fftline),df_fftline).sum()/np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum()*df_fftline.sum())\n",
    "                    #----------  timefreq-domain feature,12\n",
    "                    #5级小波变换，最后输出6个能量特征和其归一化能量特征\n",
    "                    cA5, cD5, cD4, cD3, cD2, cD1 = wavedec(df[i], 'db10', level=5)\n",
    "                    ener_cA5 = np.square(cA5).sum()\n",
    "                    ener_cD5 = np.square(cD5).sum()\n",
    "                    ener_cD4 = np.square(cD4).sum()\n",
    "                    ener_cD3 = np.square(cD3).sum()\n",
    "                    ener_cD2 = np.square(cD2).sum()\n",
    "                    ener_cD1 = np.square(cD1).sum()\n",
    "                    ener = ener_cA5 + ener_cD1 + ener_cD2 + ener_cD3 + ener_cD4 + ener_cD5\n",
    "                    ratio_cA5 = ener_cA5/ener\n",
    "                    ratio_cD5 = ener_cD5/ener\n",
    "                    ratio_cD4 = ener_cD4/ener\n",
    "                    ratio_cD3 = ener_cD3/ener\n",
    "                    ratio_cD2 = ener_cD2/ener\n",
    "                    ratio_cD1 = ener_cD1/ener\n",
    "                    feature_list.extend([time_mean,time_std,time_max,time_min,time_rms,time_ptp,time_median,time_iqr,time_pr,time_skew,time_kurtosis,time_var,time_amp,\n",
    "                                     time_smr,time_wavefactor,time_peakfactor,time_pulse,time_margin,freq_mean,freq_std,freq_max,freq_min,freq_rms,freq_median,\n",
    "                                     freq_iqr,freq_pr,freq_f2,freq_f3,freq_f4,freq_f5,freq_f6,freq_f7,freq_f8,ener_cA5,ener_cD1,ener_cD2,ener_cD3,ener_cD4,ener_cD5,\n",
    "                                     ratio_cA5,ratio_cD1,ratio_cD2,ratio_cD3,ratio_cD4,ratio_cD5])\n",
    "                features_list.append(feature_list)\n",
    "        return features_list\n",
    "\n",
    "# 约定normal(NORMAL), ball(B), outer race(OR), inner race(IR)的预测输出标签为0, 1, 2, 3。\n",
    "# NORMAL:0\n",
    "for i in range(1,3):\n",
    "    params['data_path'] = 'data/train/NORMAL0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/NORMAL0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=0\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "\n",
    "# B:1    \n",
    "for i in range(1,7):\n",
    "    params['data_path'] = 'data/train/B0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/B0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=1\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "# OR:2    \n",
    "for i in range(1,15):\n",
    "    if(i<=9):\n",
    "        params['data_path'] = 'data/train/OR0'+str(i)+'.csv'\n",
    "        params['opath'] ='datafeature/OR0'+str(i)+'_feature.csv'\n",
    "    else:\n",
    "        params['data_path'] = 'data/train/OR'+str(i)+'.csv'\n",
    "        params['opath'] ='datafeature/OR'+str(i)+'_feature.csv'        \n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=2\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "# IR:3\n",
    "for i in range(1,7):\n",
    "    params['data_path'] = 'data/train/IR0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/IR0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=3\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "\n",
    "# TEST:-1\n",
    "for i in range(1,15):\n",
    "    if(i<=9):\n",
    "        params['data_path'] = 'data/test/TEST0'+str(i)+'.csv'\n",
    "        params['opath'] ='testfeature/TEST0'+str(i)+'_feature.csv'\n",
    "    else:\n",
    "        params['data_path'] = 'data/test/TEST'+str(i)+'.csv'\n",
    "        params['opath'] ='testfeature/TEST'+str(i)+'_feature.csv'        \n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=-1\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'datafeature/B01_feature.csv'  #特征提取后的csv文件路径\n",
    "df = pd.DataFrame(pd.read_csv(path))\n",
    "delete_features = [] #需要删除的列名自行加到数组里\n",
    "df = df.drop(delete_features, axis=1) #特征选择之后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('datafeature/B01_feature.csv')\n",
    "df2 = pd.read_csv('datafeature/B02_feature.csv')\n",
    "df3 = pd.read_csv('datafeature/B03_feature.csv')\n",
    "df4 = pd.read_csv('datafeature/B04_feature.csv')\n",
    "df5 = pd.read_csv('datafeature/B05_feature.csv')\n",
    "df_fault = pd.concat([df1,df2,df3,df4,df5])\n",
    "df_fault = df_fault.reset_index(drop=True)\n",
    "df_fault.to_csv('data_faultB.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "filePath='datafeature'\n",
    "outputfile='train.csv'\n",
    "fileList = os.listdir(filePath)\n",
    "# print(fileList)\n",
    "# print(len(fileList))\n",
    "path = os.path.join(filePath, fileList[0])\n",
    "df = pd.read_csv(path)\n",
    "df.to_csv(outputfile,index=False, header=True)\n",
    "for i in range (1,len(fileList)):\n",
    "    path = os.path.join(filePath, fileList[i])\n",
    "#     print(path)\n",
    "    df = pd.read_csv(path)\n",
    "    df.to_csv(outputfile, mode='a', index=False, header=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    model_lgb_default = lgb.LGBMClassifier(learning_rate=0.03,random_state=None)\n",
    "    # 模型训练\n",
    "    model_lgb_default.fit(train_data_x, train_data_y)\n",
    "    joblib.dump(model_lgb_default, 'model/lightgbm_model.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数 ： {'n_neighbors': 7}\n",
      "最佳结果： 0.9340401871771147\n",
      "最佳预估器： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
      "                     weights='uniform')\n",
      "交叉验证结果 {'mean_fit_time': array([0.07780004, 0.07829189, 0.07630348, 0.07599342, 0.0755555 ]), 'std_fit_time': array([0.00186594, 0.00226864, 0.00049019, 0.0007767 , 0.00082951]), 'mean_score_time': array([0.07046503, 0.08175719, 0.08277839, 0.08691478, 0.09066445]), 'std_score_time': array([0.00176111, 0.00212599, 0.00070478, 0.0011238 , 0.00081975]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 9],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 9}], 'split0_test_score': array([0.92019431, 0.92643997, 0.92990978, 0.92852186, 0.92852186]), 'split1_test_score': array([0.92986111, 0.93055556, 0.93680556, 0.94097222, 0.93819444]), 'split2_test_score': array([0.91805556, 0.92916667, 0.93888889, 0.94305556, 0.94236111]), 'split3_test_score': array([0.91319444, 0.92638889, 0.92847222, 0.92361111, 0.92430556]), 'mean_test_score': array([0.92032636, 0.92813777, 0.93351911, 0.93404019, 0.93334574]), 'std_test_score': array([0.00606114, 0.00179203, 0.00441966, 0.00819371, 0.00724192]), 'rank_test_score': array([5, 4, 2, 1, 3])}\n",
      "准确度为： 0.9404617253948967\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_data_x, train_data_y, test_size=0.3, random_state=0)\n",
    "    # 传入到knn进行训练，得到模型\n",
    "    classifier = KNeighborsClassifier()\n",
    "    param_dict = {\"n_neighbors\":[1,3,5,7,9]}\n",
    "   #加入网格搜索和交叉验证\n",
    "    classifier = GridSearchCV(classifier,param_grid=param_dict,cv=4)\n",
    "    classifier.fit(train_x,train_y)\n",
    " \n",
    "    y_predict = classifier.predict(valid_x)\n",
    "    print(\"最佳参数 ：\",classifier.best_params_)\n",
    "    print(\"最佳结果：\",classifier.best_score_)\n",
    "    print(\"最佳预估器：\",classifier.best_estimator_)\n",
    "    print(\"交叉验证结果\",classifier.cv_results_)\n",
    "    print(\"准确度为：\", classifier.score(valid_x,valid_y))\n",
    "    joblib.dump(classifier, 'model/knn.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjw98\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Multiclass objective and metrics don't match\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Multiclass objective and metrics don't match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   1663\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1664\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1038\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    840\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[1;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m    888\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Multiclass objective and metrics don't match",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-bb578a967fde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-bb578a967fde>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m                          lambda_l2= 0)\n\u001b[0;32m     37\u001b[0m     \u001b[0mgsearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgbm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best score: %0.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters set:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    798\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    801\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreduced_valid_sets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mvalid_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_reverse_update_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams_back_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m             \u001b[0m_safe_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_DatasetUpdateParam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_dict_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Multiclass objective and metrics don't match"
     ]
    }
   ],
   "source": [
    "# 超参数搜索\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_data_x, train_data_y, test_size=0.333, random_state=0)\n",
    "    train = lgb.Dataset(train_x, train_y)\n",
    "    valid = lgb.Dataset(valid_x, valid_y, reference=train)\n",
    "    parameters = {\n",
    "              'max_depth': [15, 20, 25, 30, 35],\n",
    "              'learning_rate': [0.05, 0.1, 0.15],\n",
    "              'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_freq': [2, 4, 5, 6, 8],\n",
    "              'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "              'lambda_l2': [0, 10, 15, 35, 40],\n",
    "              'cat_smooth': [1, 10, 15, 20, 35]\n",
    "    }\n",
    "    gbm = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         objective = 'binary',\n",
    "                         metric = 'auc',\n",
    "                         verbose = 0,\n",
    "                         learning_rate = 0.01,\n",
    "                         num_leaves = 35,\n",
    "                         feature_fraction=0.8,\n",
    "                         bagging_fraction= 0.9,\n",
    "                         bagging_freq= 8,\n",
    "                         lambda_l1= 0.6,\n",
    "                         lambda_l2= 0)\n",
    "    gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "    gsearch.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = gsearch.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "#     joblib.dump(model_lgb_default, 'model/lightgbm_model.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of test is:\n",
      "test_1 [2, 142, 0, 1] 1\n",
      "The result of test is:\n",
      "test_2 [0, 145, 0, 1] 1\n",
      "The result of test is:\n",
      "test_3 [9, 103, 18, 16] 1\n",
      "The result of test is:\n",
      "test_4 [0, 0, 59, 13] 2\n",
      "The result of test is:\n",
      "test_5 [13, 16, 35, 9] 2\n",
      "The result of test is:\n",
      "test_6 [0, 0, 62, 11] 2\n",
      "The result of test is:\n",
      "test_7 [3, 25, 28, 17] 2\n",
      "The result of test is:\n",
      "test_8 [3637, 0, 0, 0] 0\n",
      "The result of test is:\n",
      "test_9 [8, 8, 39, 18] 2\n",
      "The result of test is:\n",
      "test_10 [7, 25, 40, 1] 2\n",
      "The result of test is:\n",
      "test_11 [0, 0, 71, 1] 2\n",
      "The result of test is:\n",
      "test_12 [5, 3, 41, 97] 3\n",
      "The result of test is:\n",
      "test_13 [0, 0, 4, 142] 3\n",
      "The result of test is:\n",
      "test_14 [2, 102, 40, 2] 1\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def judge(input_pred):\n",
    "    return_pred = list(np.zeros(len(input_pred)))\n",
    "    for i in range(0, len(input_pred)):\n",
    "        if (input_pred[i][0] > 0.5):\n",
    "            return_pred[i] = 0\n",
    "        elif(input_pred[i][1]>0.5):\n",
    "            return_pred[i] = 1\n",
    "        elif(input_pred[i][2]>0.5):\n",
    "            return_pred[i] = 2\n",
    "        elif(input_pred[i][3]>0.5):\n",
    "            return_pred[i] = 3\n",
    "    return return_pred\n",
    "    \n",
    "def score(predlist ,pred):\n",
    "    # score = [0.3×f1score(class1) + 0.3×f1score(class2) + 0.3×f1score(class3) + 0.1×f1score(class0)]*100\n",
    "    pass\n",
    "\n",
    "def test_lightgbm():\n",
    "    # 加载模型\n",
    "#     model = joblib.load('model/lightgbm_model.model')\n",
    "    model = joblib.load('model/knn.model')\n",
    "    test1=pd.read_csv('testfeature/TEST01_feature.csv')\n",
    "    test2=pd.read_csv('testfeature/TEST02_feature.csv')\n",
    "    test3=pd.read_csv('testfeature/TEST03_feature.csv')\n",
    "    test4=pd.read_csv('testfeature/TEST04_feature.csv')\n",
    "    test5=pd.read_csv('testfeature/TEST05_feature.csv')\n",
    "    test6=pd.read_csv('testfeature/TEST06_feature.csv')\n",
    "    test7=pd.read_csv('testfeature/TEST07_feature.csv')\n",
    "    test8=pd.read_csv('testfeature/TEST08_feature.csv')\n",
    "    test9=pd.read_csv('testfeature/TEST09_feature.csv')\n",
    "    test10=pd.read_csv('testfeature/TEST10_feature.csv')\n",
    "    test11=pd.read_csv('testfeature/TEST11_feature.csv')\n",
    "    test12=pd.read_csv('testfeature/TEST12_feature.csv')\n",
    "    test13=pd.read_csv('testfeature/TEST13_feature.csv')\n",
    "    test14=pd.read_csv('testfeature/TEST14_feature.csv')\n",
    "    i=1\n",
    "    for test_i in [test1,test2,test3,test4,test5,test6,test7,test8,test9,test10,test11,test12,test13,test14]:\n",
    "        test_name = 'test_' + str(i)\n",
    "        i+=1\n",
    "        print('The result of test is:')\n",
    "        test_j=test_i.drop(['label'], axis=1)\n",
    "        y_pred = model.predict_proba(test_j)\n",
    "        y_pred_binary = judge(y_pred)\n",
    "        predlist=[y_pred_binary.count(0), y_pred_binary.count(1),y_pred_binary.count(2),y_pred_binary.count(3)]\n",
    "        pred=predlist.index(max(predlist))\n",
    "        print(test_name,predlist ,pred)\n",
    "\n",
    "\n",
    "        \n",
    "test_lightgbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
