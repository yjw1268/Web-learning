{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DE_time', 'FE_time', 'BA_time', 'RPM']\n"
     ]
    }
   ],
   "source": [
    "# 初始化参数，获取列名\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "params = {}\n",
    "params['n_estimators'] = 100\n",
    "params['max_samples'] ='auto'\n",
    "params['contamination'] = 0.1\n",
    "params['max_features'] = 1.0\n",
    "\n",
    "params['nu'] = 0.001\n",
    "params['gamma']='auto'\n",
    "params['kernel'] ='poly'\n",
    "\n",
    "params['path'] = 'data/train/B01.csv'\n",
    "params['opath'] ='dataclean/B01.csv'\n",
    "try:\n",
    "    with open(params['path'],'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        head_row=next(reader)\n",
    "        data_attribute = []\n",
    "    for item in head_row:\n",
    "        data_attribute.append(item)\n",
    "    print(data_attribute)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.91511018e-01 -3.69818182e-02 -8.20842730e-03  1.79600000e+03]\n",
      " [ 1.50577365e-01  4.49945455e-02 -1.55316320e-02  1.79600000e+03]\n",
      " [-3.68727745e-02 -8.62909091e-03  3.82255193e-03  1.79600000e+03]\n",
      " ...\n",
      " [ 2.42353214e-01 -6.98545455e-03  7.06970920e-02  1.79600000e+03]\n",
      " [ 3.24870259e-03  1.06836364e-02  7.12604154e-02  1.79600000e+03]\n",
      " [-6.22126547e-02 -1.01700000e-01 -2.03198813e-02  1.79600000e+03]]\n",
      "14708\n"
     ]
    }
   ],
   "source": [
    "# dataclean\n",
    "tn = pd.read_csv(params['path']) \n",
    "tn.dropna(inplace=True)\n",
    "train = np.array(tn)\n",
    "train_x = np.array(train)\n",
    "print(train_x)\n",
    "# clf = IsolationForest(n_estimators=params['n_estimators'], \n",
    "#                       max_samples=params['max_samples'], \n",
    "#                       contamination=params['contamination'], \n",
    "#                       max_features=params['max_features'], \n",
    "#                       bootstrap=False, n_jobs=1, random_state=None, \n",
    "#                       verbose=0).fit(train_x)\n",
    "clf = svm.OneClassSVM(nu=params['nu'],\n",
    "              kernel=params['kernel'],\n",
    "              gamma=params['gamma']).fit(train_x)\n",
    "pred = clf.predict(train_x)\n",
    "print (pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14708\n"
     ]
    }
   ],
   "source": [
    "# 保存清洗后数据\n",
    "df = pd.DataFrame(pd.read_csv(params['path']))[0:pred.size]\n",
    "df['pred']=pred\n",
    "df2 = df[-df.pred.isin([-1])]\n",
    "df2 = df2.drop(['pred'],axis=1)\n",
    "data_out = df2.iloc[:,:].values\n",
    "print(int(df2.size/4))\n",
    "csvfile2 = open(params['opath'],'w',newline='')\n",
    "writer = csv.writer(csvfile2)\n",
    "writer.writerow(data_attribute)   #存属性\n",
    "m=len(data_out)\n",
    "for i in range(m):\n",
    "    writer.writerow(data_out[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjw98\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pywt\\_multilevel.py:45: UserWarning: Level value of 5 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362927\n",
      "14708\n",
      "14621\n",
      "14638\n",
      "14569\n",
      "14656\n",
      "14604\n",
      "7336\n",
      "7319\n",
      "7336\n",
      "7310\n",
      "7302\n",
      "7345\n",
      "7310\n",
      "7310\n",
      "7345\n",
      "7319\n",
      "7328\n",
      "7319\n",
      "7319\n",
      "7345\n",
      "14551\n",
      "14621\n",
      "14656\n",
      "14638\n",
      "14621\n",
      "14586\n",
      "14586\n",
      "14656\n",
      "14621\n",
      "7293\n",
      "7336\n",
      "7336\n",
      "7362\n",
      "363797\n",
      "7336\n",
      "7310\n",
      "7284\n",
      "14621\n",
      "14656\n",
      "14638\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 特征提取\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats,signal,fftpack\n",
    "import math\n",
    "from pywt import wavedec\n",
    "import traceback\n",
    "import sys\n",
    "columns_list=['time_mean','time_std','time_max','time_min','time_rms','time_ptp','time_median','time_iqr','time_pr','time_skew','time_kurtosis','time_var','time_amp',                'time_smr','time_wavefactor','time_peakfactor','time_pulse','time_margin','freq_mean','freq_std','freq_max','freq_min','freq_rms','freq_median',               'freq_iqr','freq_pr','freq_f2','freq_f3','freq_f4','freq_f5','freq_f6','freq_f7','freq_f8','ener_cA5','ener_cD1','ener_cD2','ener_cD3','ener_cD4',        'ener_cD5','ratio_cA5','ratio_cD1','ratio_cD2','ratio_cD3','ratio_cD4','ratio_cD5']\n",
    "columns_list1 = [a + '_DE' for a in columns_list]\n",
    "columns_list2 = [a + '_FE' for a in columns_list]\n",
    "# columns_list3 = [a + '_BA' for a in columns_list]\n",
    "# columns_list_final = columns_list1 + columns_list2+ columns_list3\n",
    "columns_list_final = columns_list1 + columns_list2\n",
    "params = {}\n",
    "params['len_piece']=50 #窗口长度\n",
    "windowlen=params['len_piece']\n",
    "\n",
    "\n",
    "def feature_get(filepath,windowlen):\n",
    "        df_data = pd.DataFrame(pd.read_csv(filepath))\n",
    "#         dfs = df_data.loc[:,['DE_time','FE_time','BA_time']]\n",
    "        dfs = df_data.loc[:,['DE_time','FE_time']]\n",
    "        features_list=[]\n",
    "        print(len(dfs))\n",
    "        for i in range (0,len(dfs),windowlen):\n",
    "            if(int((len(dfs)-i)/100)>=1): #舍去少量数据\n",
    "                df=dfs[i:i+params['len_piece']]\n",
    "                feature_list = []\n",
    "    #             print(df)\n",
    "                for i in df.columns:\n",
    "                    #----------  time-domain feature,18\n",
    "                    #依次为均值，标准差，最大值，最小值，均方根，峰峰值，中位数，四分位差，百分位差，偏度，峰度，方差，整流平均值，方根幅值，波形因子，峰值因子，脉冲值，裕度\n",
    "                    df_line = df[i]\n",
    "    #                 print(df_line)\n",
    "                    time_mean = df_line.mean()\n",
    "                    time_std = df_line.std()\n",
    "                    time_max = df_line.max()\n",
    "                    time_min = df_line.min()\n",
    "                    time_rms = np.sqrt(np.square(df_line).mean())\n",
    "                    time_ptp = time_max-time_min \n",
    "                    time_median = np.median(df_line)\n",
    "                    time_iqr = np.percentile(df_line,75)-np.percentile(df_line,25)\n",
    "                    time_pr = np.percentile(df_line,90)-np.percentile(df_line,10)\n",
    "                    time_skew = stats.skew(df_line)\n",
    "                    time_kurtosis = stats.kurtosis(df_line)\n",
    "                    time_var = np.var(df_line)\n",
    "                    time_amp = np.abs(df_line).mean()\n",
    "                    time_smr = np.square(np.sqrt(np.abs(df_line)).mean())\n",
    "                    #下面四个特征需要注意分母为0或接近0问题，可能会发生报错\n",
    "                    time_wavefactor = time_rms/time_amp\n",
    "                    time_peakfactor = time_max/time_rms\n",
    "                    time_pulse = time_max/time_amp\n",
    "                    time_margin = time_max/time_smr\n",
    "                    #----------  freq-domain feature,15\n",
    "                    #采样频率25600Hz\n",
    "                    df_fftline = fftpack.fft(df[i])\n",
    "                    freq_fftline = fftpack.fftfreq(len(df[i]),1/25600)\n",
    "                    df_fftline = abs(df_fftline[freq_fftline>=0])\n",
    "                    freq_fftline = freq_fftline[freq_fftline>=0]\n",
    "                    #基本特征,依次为均值，标准差，最大值，最小值，均方根，中位数，四分位差，百分位差\n",
    "                    freq_mean = df_fftline.mean()\n",
    "                    freq_std = df_fftline.std()\n",
    "                    freq_max = df_fftline.max()\n",
    "                    freq_min = df_fftline.min()\n",
    "                    freq_rms = np.sqrt(np.square(df_fftline).mean())\n",
    "                    freq_median = np.median(df_fftline)\n",
    "                    freq_iqr = np.percentile(df_fftline,75)-np.percentile(df_fftline,25)\n",
    "                    freq_pr = np.percentile(df_fftline,90)-np.percentile(df_fftline,10)\n",
    "                    #f2 f3 f4反映频谱集中程度\n",
    "                    freq_f2 = np.square((df_fftline-freq_mean)).sum()/(len(df_fftline)-1)\n",
    "                    freq_f3 = pow((df_fftline-freq_mean),3).sum()/(len(df_fftline)*pow(freq_f2,1.5))\n",
    "                    freq_f4 = pow((df_fftline-freq_mean),4).sum()/(len(df_fftline)*pow(freq_f2,2))\n",
    "                    #f5 f6 f7 f8反映主频带位置\n",
    "                    freq_f5 = np.multiply(freq_fftline,df_fftline).sum()/df_fftline.sum()\n",
    "                    freq_f6 = np.sqrt(np.multiply(np.square(freq_fftline),df_fftline).sum())/df_fftline.sum()\n",
    "                    freq_f7 = np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum())/np.multiply(np.square(freq_fftline),df_fftline).sum()\n",
    "                    freq_f8 = np.multiply(np.square(freq_fftline),df_fftline).sum()/np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum()*df_fftline.sum())\n",
    "                    #----------  timefreq-domain feature,12\n",
    "                    #5级小波变换，最后输出6个能量特征和其归一化能量特征\n",
    "                    cA5, cD5, cD4, cD3, cD2, cD1 = wavedec(df[i], 'db10', level=5)\n",
    "                    ener_cA5 = np.square(cA5).sum()\n",
    "                    ener_cD5 = np.square(cD5).sum()\n",
    "                    ener_cD4 = np.square(cD4).sum()\n",
    "                    ener_cD3 = np.square(cD3).sum()\n",
    "                    ener_cD2 = np.square(cD2).sum()\n",
    "                    ener_cD1 = np.square(cD1).sum()\n",
    "                    ener = ener_cA5 + ener_cD1 + ener_cD2 + ener_cD3 + ener_cD4 + ener_cD5\n",
    "                    ratio_cA5 = ener_cA5/ener\n",
    "                    ratio_cD5 = ener_cD5/ener\n",
    "                    ratio_cD4 = ener_cD4/ener\n",
    "                    ratio_cD3 = ener_cD3/ener\n",
    "                    ratio_cD2 = ener_cD2/ener\n",
    "                    ratio_cD1 = ener_cD1/ener\n",
    "                    feature_list.extend([time_mean,time_std,time_max,time_min,time_rms,time_ptp,time_median,time_iqr,time_pr,time_skew,time_kurtosis,time_var,time_amp,\n",
    "                                     time_smr,time_wavefactor,time_peakfactor,time_pulse,time_margin,freq_mean,freq_std,freq_max,freq_min,freq_rms,freq_median,\n",
    "                                     freq_iqr,freq_pr,freq_f2,freq_f3,freq_f4,freq_f5,freq_f6,freq_f7,freq_f8,ener_cA5,ener_cD1,ener_cD2,ener_cD3,ener_cD4,ener_cD5,\n",
    "                                     ratio_cA5,ratio_cD1,ratio_cD2,ratio_cD3,ratio_cD4,ratio_cD5])\n",
    "                features_list.append(feature_list)\n",
    "        return features_list\n",
    "\n",
    "# 约定normal(NORMAL), ball(B), outer race(OR), inner race(IR)的预测输出标签为0, 1, 2, 3。\n",
    "# NORMAL:0\n",
    "for i in range(1,3):\n",
    "    params['data_path'] = 'data/train/NORMAL0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/NORMAL0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=0\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "\n",
    "# B:1    \n",
    "for i in range(1,7):\n",
    "    params['data_path'] = 'data/train/B0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/B0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=1\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "# OR:2    \n",
    "for i in range(1,15):\n",
    "    if(i<=9):\n",
    "        params['data_path'] = 'data/train/OR0'+str(i)+'.csv'\n",
    "        params['opath'] ='datafeature/OR0'+str(i)+'_feature.csv'\n",
    "    else:\n",
    "        params['data_path'] = 'data/train/OR'+str(i)+'.csv'\n",
    "        params['opath'] ='datafeature/OR'+str(i)+'_feature.csv'        \n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=2\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "# IR:3\n",
    "for i in range(1,7):\n",
    "    params['data_path'] = 'data/train/IR0'+str(i)+'.csv'\n",
    "    params['opath'] ='datafeature/IR0'+str(i)+'_feature.csv'\n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=3\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "    \n",
    "\n",
    "# TEST:-1\n",
    "for i in range(1,15):\n",
    "    if(i<=9):\n",
    "        params['data_path'] = 'data/test/TEST0'+str(i)+'.csv'\n",
    "        params['opath'] ='testfeature/TEST0'+str(i)+'_feature.csv'\n",
    "    else:\n",
    "        params['data_path'] = 'data/test/TEST'+str(i)+'.csv'\n",
    "        params['opath'] ='testfeature/TEST'+str(i)+'_feature.csv'        \n",
    "    fea = feature_get(params['data_path'],windowlen)\n",
    "    result = pd.DataFrame(fea,columns = columns_list_final)\n",
    "    result['label']=-1\n",
    "    result.to_csv(params['opath'],index=False,header=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 合并训练集\n",
    "import os\n",
    "import pandas as pd\n",
    "filePath='datafeature'\n",
    "outputfile='train.csv'\n",
    "fileList = os.listdir(filePath)\n",
    "# print(fileList)\n",
    "# print(len(fileList))\n",
    "path = os.path.join(filePath, fileList[0])\n",
    "df = pd.read_csv(path)\n",
    "df.to_csv(outputfile,index=False, header=True)\n",
    "for i in range (1,len(fileList)):\n",
    "    path = os.path.join(filePath, fileList[i])\n",
    "#     print(path)\n",
    "    df = pd.read_csv(path)\n",
    "    df.to_csv(outputfile, mode='a', index=False, header=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# LGBM分类\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    model_lgb_default = lgb.LGBMClassifier(learning_rate=0.03,random_state=None)\n",
    "    # 模型训练\n",
    "    model_lgb_default.fit(train_data_x, train_data_y)\n",
    "    joblib.dump(model_lgb_default, 'model/lightgbm_model.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数 ： {'n_neighbors': 7}\n",
      "最佳结果： 0.9106550658597297\n",
      "最佳预估器： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
      "                     weights='uniform')\n",
      "交叉验证结果 {'mean_fit_time': array([0.17730254, 0.17129713, 0.17153877, 0.17227316, 0.17377406]), 'std_fit_time': array([0.00310815, 0.00130852, 0.00157492, 0.00042172, 0.003342  ]), 'mean_score_time': array([0.17852104, 0.20121932, 0.21692711, 0.23238713, 0.24963623]), 'std_score_time': array([0.00612883, 0.00658459, 0.00631842, 0.00435995, 0.00884237]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 9],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 9}], 'split0_test_score': array([0.89746263, 0.90858533, 0.91067084, 0.9092805 , 0.91032325]), 'split1_test_score': array([0.89016336, 0.90823775, 0.90962808, 0.91484185, 0.91206117]), 'split2_test_score': array([0.88734353, 0.89986092, 0.90716273, 0.91376912, 0.91307371]), 'split3_test_score': array([0.89394993, 0.8988178 , 0.90646732, 0.90472879, 0.90472879]), 'mean_test_score': array([0.89222987, 0.90387545, 0.90848224, 0.91065507, 0.91004673]), 'std_test_score': array([0.00382385, 0.00455272, 0.00172511, 0.00400739, 0.00322403]), 'rank_test_score': array([5, 4, 3, 1, 2])}\n",
      "准确度为： 0.910786699107867\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# knn分类\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_data_x, train_data_y, test_size=0.3, random_state=0)\n",
    "    # 传入到knn进行训练，得到模型\n",
    "    classifier = KNeighborsClassifier()\n",
    "    param_dict = {\"n_neighbors\":[1,3,5,7,9]}\n",
    "   #加入网格搜索和交叉验证\n",
    "    classifier = GridSearchCV(classifier,param_grid=param_dict,cv=4)\n",
    "    classifier.fit(train_x,train_y)\n",
    " \n",
    "    y_predict = classifier.predict(valid_x)\n",
    "    print(\"最佳参数 ：\",classifier.best_params_)\n",
    "    print(\"最佳结果：\",classifier.best_score_)\n",
    "    print(\"最佳预估器：\",classifier.best_estimator_)\n",
    "    print(\"交叉验证结果\",classifier.cv_results_)\n",
    "    print(\"准确度为：\", classifier.score(valid_x,valid_y))\n",
    "    joblib.dump(classifier, 'model/knn.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjw98\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Multiclass objective and metrics don't match\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   1663\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1664\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1038\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    840\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[1;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m    888\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Multiclass objective and metrics don't match",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fa0b8827293e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-fa0b8827293e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m                          lambda_l2= 0)\n\u001b[0;32m     49\u001b[0m     \u001b[0mgsearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgbm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best score: %0.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters set:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    798\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    801\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreduced_valid_sets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mvalid_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_reverse_update_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams_back_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m             \u001b[0m_safe_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_DatasetUpdateParam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_dict_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 超参数搜索（BUGing）\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train():\n",
    "    # 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    train_data_y = train_data['label']\n",
    "    # 除去标签的所有列就是特征\n",
    "    train_data_x = train_data.drop(['label'], axis=1)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_data_x, train_data_y, test_size=0.333, random_state=0)\n",
    "    train = lgb.Dataset(train_x, train_y)\n",
    "    valid = lgb.Dataset(valid_x, valid_y, reference=train)\n",
    "    parameters = {\n",
    "              'max_depth': [15, 20, 25, 30, 35],\n",
    "              'learning_rate': [0.05, 0.1, 0.15],\n",
    "              'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "              'bagging_freq': [2, 4, 5, 6, 8],\n",
    "              'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "              'lambda_l2': [0, 10, 15, 35, 40],\n",
    "              'cat_smooth': [1, 10, 15, 20, 35]\n",
    "    }\n",
    "    gbm = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         objective = 'binary',\n",
    "                         metric = 'auc',\n",
    "                         verbose = 0,\n",
    "                         learning_rate = 0.01,\n",
    "                         num_leaves = 35,\n",
    "                         feature_fraction=0.8,\n",
    "                         bagging_fraction= 0.9,\n",
    "                         bagging_freq= 8,\n",
    "                         lambda_l1= 0.6,\n",
    "                         lambda_l2= 0)\n",
    "    gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "    gsearch.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = gsearch.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "#     joblib.dump(model_lgb_default, 'model/lightgbm_model.model')\n",
    "    print(\"Done.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\yjw98\\\\AppData\\\\Local\\\\Temp\\\\tmpr7d8sfnq', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AE89B5BDD8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt.\n",
      "INFO:tensorflow:loss = 477.38135, step = 0\n",
      "INFO:tensorflow:global_step/sec: 34.127\n",
      "INFO:tensorflow:loss = 26.007673, step = 100 (2.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.9628\n",
      "INFO:tensorflow:loss = 12.8824215, step = 200 (2.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.5137\n",
      "INFO:tensorflow:loss = 7.9573746, step = 300 (2.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.2892\n",
      "INFO:tensorflow:loss = 4.509885, step = 400 (2.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4599\n",
      "INFO:tensorflow:loss = 4.399679, step = 500 (2.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.7047\n",
      "INFO:tensorflow:loss = 5.328124, step = 600 (2.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.549\n",
      "INFO:tensorflow:loss = 3.3076088, step = 700 (2.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3426\n",
      "INFO:tensorflow:loss = 2.7833052, step = 800 (2.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.1677\n",
      "INFO:tensorflow:loss = 2.6701884, step = 900 (2.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.6379\n",
      "INFO:tensorflow:loss = 2.225646, step = 1000 (2.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.3085\n",
      "INFO:tensorflow:loss = 2.1079993, step = 1100 (2.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.728\n",
      "INFO:tensorflow:loss = 2.125925, step = 1200 (2.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.7246\n",
      "INFO:tensorflow:loss = 1.475898, step = 1300 (2.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2613\n",
      "INFO:tensorflow:loss = 1.6210945, step = 1400 (2.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2495\n",
      "INFO:tensorflow:loss = 0.9851977, step = 1500 (2.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.1439\n",
      "INFO:tensorflow:loss = 1.7039717, step = 1600 (2.846 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2679\n",
      "INFO:tensorflow:loss = 1.1018794, step = 1700 (2.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.5934\n",
      "INFO:tensorflow:loss = 1.2961287, step = 1800 (2.809 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4043\n",
      "INFO:tensorflow:loss = 1.4143274, step = 1900 (2.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.3737\n",
      "INFO:tensorflow:loss = 1.2451428, step = 2000 (2.827 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8867\n",
      "INFO:tensorflow:loss = 1.3620313, step = 2100 (2.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8756\n",
      "INFO:tensorflow:loss = 1.1556408, step = 2200 (2.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.1504\n",
      "INFO:tensorflow:loss = 1.7966553, step = 2300 (2.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8028\n",
      "INFO:tensorflow:loss = 1.073757, step = 2400 (2.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.5675\n",
      "INFO:tensorflow:loss = 1.1186054, step = 2500 (2.812 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6934\n",
      "INFO:tensorflow:loss = 0.82921785, step = 2600 (2.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.499\n",
      "INFO:tensorflow:loss = 0.9315698, step = 2700 (2.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.1316\n",
      "INFO:tensorflow:loss = 0.603294, step = 2800 (2.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.9302\n",
      "INFO:tensorflow:loss = 0.99318564, step = 2900 (2.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.386\n",
      "INFO:tensorflow:loss = 0.87314504, step = 3000 (2.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6188\n",
      "INFO:tensorflow:loss = 0.90249646, step = 3100 (2.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.549\n",
      "INFO:tensorflow:loss = 0.9838475, step = 3200 (2.813 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2087\n",
      "INFO:tensorflow:loss = 0.79669636, step = 3300 (2.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.0761\n",
      "INFO:tensorflow:loss = 0.7521049, step = 3400 (2.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.5302\n",
      "INFO:tensorflow:loss = 0.7351097, step = 3500 (2.814 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7775\n",
      "INFO:tensorflow:loss = 0.6462076, step = 3600 (2.796 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6252\n",
      "INFO:tensorflow:loss = 0.5800092, step = 3700 (2.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4551\n",
      "INFO:tensorflow:loss = 0.8017358, step = 3800 (2.821 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5987\n",
      "INFO:tensorflow:loss = 0.54024607, step = 3900 (2.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.9852\n",
      "INFO:tensorflow:loss = 0.6000339, step = 4000 (2.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.3299\n",
      "INFO:tensorflow:loss = 0.46087644, step = 4100 (2.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6463\n",
      "INFO:tensorflow:loss = 0.5087275, step = 4200 (2.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9703\n",
      "INFO:tensorflow:loss = 0.6569883, step = 4300 (2.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4586\n",
      "INFO:tensorflow:loss = 0.54969853, step = 4400 (2.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6818\n",
      "INFO:tensorflow:loss = 0.49456006, step = 4500 (2.803 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9378\n",
      "INFO:tensorflow:loss = 0.5212704, step = 4600 (2.783 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7205\n",
      "INFO:tensorflow:loss = 0.6351094, step = 4700 (2.800 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4987\n",
      "INFO:tensorflow:loss = 0.67432094, step = 4800 (2.818 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4382\n",
      "INFO:tensorflow:loss = 0.4288397, step = 4900 (2.903 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.54979104.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x1ae804dbcc0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to use tensorflow DNN\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "    # 将输入转换为数据集。\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # 如果在训练模式下混淆并重复数据。\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "# 模型初始化，设置random_state保证可复现性，便于观察优化\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data_y = train_data['label']\n",
    "# 除去标签的所有列就是特征\n",
    "train_data_x = train_data.drop(['label'], axis=1)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_data_x, train_data_y, test_size=0.25, random_state=0)\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # 隐层所含结点数量分别为 30 和 10.\n",
    "    hidden_units=[30, 10],\n",
    "    # 模型必须从三个类别中做出选择。\n",
    "    n_classes=4)\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train_x, train_y, training=True),\n",
    "    steps=5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-03-25T21:15:23Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-03-25-21:15:24\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.8452555, average_loss = 0.6445051, global_step = 5000, loss = 0.6268315\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "\n",
      "Test set accuracy: 0.845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估train_test_split中划分的测试集\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(test_x, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\yjw98\\AppData\\Local\\Temp\\tmpr7d8sfnq\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "test_1 [29, 232, 26, 3] 1\n",
      "test_2 [5, 280, 4, 3] 1\n",
      "test_3 [3, 198, 56, 34] 1\n",
      "test_4 [0, 4, 125, 15] 2\n",
      "test_5 [0, 38, 66, 41] 2\n",
      "test_6 [1, 11, 70, 63] 2\n",
      "test_7 [6, 81, 28, 31] 1\n",
      "test_8 [7267, 3, 4, 0] 0\n",
      "test_9 [0, 44, 70, 31] 2\n",
      "test_10 [0, 99, 4, 42] 1\n",
      "test_11 [0, 24, 39, 81] 3\n",
      "test_12 [0, 46, 160, 85] 2\n",
      "test_13 [0, 12, 54, 226] 3\n",
      "test_14 [51, 226, 14, 0] 1\n"
     ]
    }
   ],
   "source": [
    "# 由tf模型生成预测\n",
    "expected = ['NORMAL', 'B', 'OR','IR']\n",
    "test1=pd.read_csv('testfeature/TEST01_feature.csv')\n",
    "test2=pd.read_csv('testfeature/TEST02_feature.csv')\n",
    "test3=pd.read_csv('testfeature/TEST03_feature.csv')\n",
    "test4=pd.read_csv('testfeature/TEST04_feature.csv')\n",
    "test5=pd.read_csv('testfeature/TEST05_feature.csv')\n",
    "test6=pd.read_csv('testfeature/TEST06_feature.csv')\n",
    "test7=pd.read_csv('testfeature/TEST07_feature.csv')\n",
    "test8=pd.read_csv('testfeature/TEST08_feature.csv')\n",
    "test9=pd.read_csv('testfeature/TEST09_feature.csv')\n",
    "test10=pd.read_csv('testfeature/TEST10_feature.csv')\n",
    "test11=pd.read_csv('testfeature/TEST11_feature.csv')\n",
    "test12=pd.read_csv('testfeature/TEST12_feature.csv')\n",
    "test13=pd.read_csv('testfeature/TEST13_feature.csv')\n",
    "test14=pd.read_csv('testfeature/TEST14_feature.csv')\n",
    "def input_fn(features, batch_size=256):\n",
    "    \"\"\"An input function for prediction.\"\"\"\n",
    "    # 将输入转换为无标签数据集。\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "i=1\n",
    "testsets=[test1,test2,test3,test4,test5,test6,test7,test8,test9,test10,test11,test12,test13,test14]\n",
    "names=[]\n",
    "predlists=[]\n",
    "ans=[]\n",
    "for test_i in testsets:\n",
    "    test_name = 'test_' + str(i)\n",
    "    i+=1\n",
    "    test_j=test_i.drop(['label'], axis=1)\n",
    "    predict_x=test_i\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda: input_fn(predict_x))\n",
    "    y_pred_binary=[]\n",
    "    for pred in predictions:\n",
    "        y_pred_binary.append(pred['class_ids'][0])\n",
    "    predlist=[y_pred_binary.count(0), y_pred_binary.count(1),y_pred_binary.count(2),y_pred_binary.count(3)]\n",
    "    pred=predlist.index(max(predlist))\n",
    "    names.append(test_name)\n",
    "    ans.append(pred)\n",
    "    predlists.append(predlist)\n",
    "    \n",
    "for name,predlist,pred in zip(names,predlists,ans):\n",
    "    print(name,predlist,pred)\n",
    "    \n",
    "# for pred_dict, expec in zip(predictions, expected):\n",
    "#     class_id = pred_dict['class_ids'][0]\n",
    "#     probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "#     print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\n",
    "#         expected[class_id], 100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of test is:\n",
      "test_1 [3, 286, 1, 0] 1\n",
      "The result of test is:\n",
      "test_2 [22, 260, 6, 4] 1\n",
      "The result of test is:\n",
      "test_3 [36, 198, 28, 29] 1\n",
      "The result of test is:\n",
      "test_4 [3, 0, 85, 56] 2\n",
      "The result of test is:\n",
      "test_5 [19, 50, 68, 8] 2\n",
      "The result of test is:\n",
      "test_6 [1, 1, 98, 45] 2\n",
      "The result of test is:\n",
      "test_7 [12, 61, 52, 21] 1\n",
      "The result of test is:\n",
      "test_8 [7272, 2, 0, 0] 0\n",
      "The result of test is:\n",
      "test_9 [18, 41, 69, 17] 2\n",
      "The result of test is:\n",
      "test_10 [19, 44, 78, 4] 2\n",
      "The result of test is:\n",
      "test_11 [11, 4, 117, 12] 2\n",
      "The result of test is:\n",
      "test_12 [29, 17, 83, 162] 3\n",
      "The result of test is:\n",
      "test_13 [0, 0, 8, 284] 3\n",
      "The result of test is:\n",
      "test_14 [48, 219, 20, 4] 1\n"
     ]
    }
   ],
   "source": [
    "# 预测结果\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def judge(input_pred):\n",
    "    return_pred = list(np.zeros(len(input_pred)))\n",
    "    for i in range(0, len(input_pred)):\n",
    "        if (input_pred[i][0] > 0.5):\n",
    "            return_pred[i] = 0\n",
    "        elif(input_pred[i][1]>0.5):\n",
    "            return_pred[i] = 1\n",
    "        elif(input_pred[i][2]>0.5):\n",
    "            return_pred[i] = 2\n",
    "        elif(input_pred[i][3]>0.5):\n",
    "            return_pred[i] = 3\n",
    "    return return_pred\n",
    "    \n",
    "def score(predlist ,pred):\n",
    "    # score = [0.3×f1score(class1) + 0.3×f1score(class2) + 0.3×f1score(class3) + 0.1×f1score(class0)]*100\n",
    "    pass\n",
    "\n",
    "def test_lightgbm():\n",
    "    # 加载模型\n",
    "#     model = joblib.load('model/lightgbm_model.model')\n",
    "    model = joblib.load('model/knn.model')\n",
    "    test1=pd.read_csv('testfeature/TEST01_feature.csv')\n",
    "    test2=pd.read_csv('testfeature/TEST02_feature.csv')\n",
    "    test3=pd.read_csv('testfeature/TEST03_feature.csv')\n",
    "    test4=pd.read_csv('testfeature/TEST04_feature.csv')\n",
    "    test5=pd.read_csv('testfeature/TEST05_feature.csv')\n",
    "    test6=pd.read_csv('testfeature/TEST06_feature.csv')\n",
    "    test7=pd.read_csv('testfeature/TEST07_feature.csv')\n",
    "    test8=pd.read_csv('testfeature/TEST08_feature.csv')\n",
    "    test9=pd.read_csv('testfeature/TEST09_feature.csv')\n",
    "    test10=pd.read_csv('testfeature/TEST10_feature.csv')\n",
    "    test11=pd.read_csv('testfeature/TEST11_feature.csv')\n",
    "    test12=pd.read_csv('testfeature/TEST12_feature.csv')\n",
    "    test13=pd.read_csv('testfeature/TEST13_feature.csv')\n",
    "    test14=pd.read_csv('testfeature/TEST14_feature.csv')\n",
    "    i=1\n",
    "    for test_i in [test1,test2,test3,test4,test5,test6,test7,test8,test9,test10,test11,test12,test13,test14]:\n",
    "        test_name = 'test_' + str(i)\n",
    "        i+=1\n",
    "        print('The result of test is:')\n",
    "        test_j=test_i.drop(['label'], axis=1)\n",
    "        y_pred = model.predict_proba(test_j)\n",
    "        y_pred_binary = judge(y_pred)\n",
    "        predlist=[y_pred_binary.count(0), y_pred_binary.count(1),y_pred_binary.count(2),y_pred_binary.count(3)]\n",
    "        pred=predlist.index(max(predlist))\n",
    "        print(test_name,predlist ,pred)\n",
    "\n",
    "\n",
    "        \n",
    "test_lightgbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
